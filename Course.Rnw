\documentclass{article}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}

\newtheorem{lemma}{Lemma}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}


\title{Computational Statistics, M1 MAS DS, Aix-Marseille University}
\author{Houssam BOUKHECHAM}


\begin{document}

\maketitle
\tableofcontents
% Here is an example R code:

% <<echo=TRUE>>=
% x <- c(1, 2, 3, 4, 5)
% mean(x)
% @

\newpage
%%%%%%%%%%%%
\section{Random variable simulation}

% TODO: A small introduction 
\cite{RobertCasela1999MonteCarloSM, gentle2009computational, tokdar2010importance}

\subsection{Transformation methods}

\begin{lemma}
  Let $F:\mathbb{R} \to [0,1]$ be an non-decreasing function. 
  If a random variable $X$ has $F$ as its cumulative distribution function (CDF), then the random variable $U = F(X) \sim U(0,1)$.
\end{lemma}
  
\begin{proof}
  % TODO
\end{proof}


\begin{example}[Normal variable generation]

  The cumulative distribution function (CDF) of a Gaussian random variable with mean $\mu$ and standard deviation $\sigma$ is given by:  
  \begin{equation}\label{CDF of a Gaussian(mu,sigma)}
  F(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^x e^{-\frac{1}{2}\left(\frac{t-\mu}{\sigma}\right)^2}~dt.
  \end{equation}  
  The function $F$ is a diffeomorphism. Assuming $\mu = 0$ and $\sigma = 1$, an approximation $F_a^{-1}$ of the inverse function $F^{-1}$ can be computed to arbitrary precision (\cite{see RC MCSM}, Example 2.6). To generate a random sample of size $n$ from the standard Gaussian distribution, we first generate $n$ random samples from the uniform distribution, $\{u_1, u_2, \ldots, u_n\}$. Then, we map this sample to the Gaussian distribution using the inverse approximation :  
  \[
  \left\{F_a^{-1}(u_1), F_a^{-1}(u_2), \ldots, F_a^{-1}(u_n)\right\}.
  \]
  
  \end{example}
  
  \begin{exercise}
  \begin{enumerate}
  \item[] 
  \item Generate a sample $\mathcal{S} = \left\{u_1, u_2, \cdots, u_n\right\}$ of size $n = 500$ from the uniform distribution.
  
  \item Implement the function
  \[
  F_a^{-1}(u) = t - \frac{a_0 + a_1t}{1+b_1t+b_2t^2}, \quad u\in(0,1),
  \]
  where $t^2 = \log\left(u^{-2}\right)$ and $a_0 = 2.30753, a_1 = 0.27061, b_1 = 0.99229, b_2 = 0.04481$
  
  \item Plot the histogram of the set $F_a^{-1}(\mathcal{S})$ and comment the results.
  
  \item$\clubsuit$ Repeat the process for a larger value of $n$ (e.g., $n = 50000$). Compare the generated sample with a standard Gaussian random variable generator and comment the results.
  
  \end{enumerate}
  \end{exercise}

\subsection{Accept-reject method}

\newpage
%%%%%%%%%%%%
\section{Importance sampling}

<<echo=TRUE>>=
set.seed(12) 

# Define the target distribution (Gamma distribution)
target_dist <- function(x, shape = 2, rate = 1) {
  ifelse(x > 0, x^(shape - 1) * exp(-rate * x), 0)
}

# Define the proposal distribution (Normal distribution)
proposal_dist <- function(x) {
  dnorm(x, mean = 2, sd = 2)  # Normal(2, 2) PDF
}
@

<<echo=TRUE>>=
# Generate samples from the proposal distribution 
n_samples <- 2000

# proposal sample
ps <- rnorm(n_samples, mean = 2, sd = 2)

# Compute the weights for each sample
weights <- target_dist(ps) / proposal_dist(ps)

# Estimate the mean of the target distribution using IS
importance_sampling_mean <- sum(weights * ps) / sum(weights)
print(importance_sampling_mean)
@

%%%%%%%%%%%
\section{Bootstrap}

%%%
\begin{exercise}
Let $S = \lbrace x_1, x_2, \ldots, x_n \rbrace$ be a random sample from the uniform distribution on the interval $(0, \theta)$. Assume we want to estimate the unknown parameter $\theta$, so we use the estimator $X_{(n)} = \max x_i$.

\begin{enumerate}
\item If $X_1,X_2,\cdots, X_n$ are iid with uniform distribution on $(0,\theta),$ what is the distribution of the random variable $X_{(n)} = \max X_i$? 

(Hint: Determine the cumulative distribution function of $X_{(n)}$)

\item Is the estimator $X_{(n)}$ biased?

\item How would you use the bootstrap to estimate the bias in $X_{(n)}$ for $\theta$?
\end{enumerate}

\end{exercise}

%%%%%%%%%%%%
\section{Monte Carlo methods}

%%%%%%%%%%%
\section{Gibbs algorithms for bayesian statistics}

\subsection{Metropolis Hastings algorithm}
% TODO

\subsection{MCMC}
% TODO

% Bibliography 

\newpage
%\bibliography{bibliography.bib}
bibliography
\end{document}



